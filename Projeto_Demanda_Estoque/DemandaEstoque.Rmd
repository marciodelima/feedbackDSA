---
title: "Projeto 2 - FeedBack -> Prevendo Demanda de Estoque"
author: "Marcio de Lima"
date: "16 de maio de 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Setando Diretorio
setwd("~/Cursos_DSA/FCD/BigData_R_Azure/FeedBack/Projeto_Demanda_Estoque")

```

##Prevendo Demanda de Estoque

Em resumo, Neste projeto de aprendizado de máquina, você deve desenvolver um modelo para prever com precisão a demanda de estoque com base nos dados históricos de vendas.

##DataSet
https://www.kaggle.com/c/grupo-bimbo-inventory-demand

## Dicionario de Dados - Descricao das Colunas
 * Semana — Week number (From Thursday to Wednesday)
 * Agencia_ID — Sales Depot ID
 * Canal_ID — Sales Channel ID
 * Ruta_SAK — Route ID (Several routes = Sales Depot)
 * Cliente_ID — Client ID
 * NombreCliente — Client name
 * Producto_ID — Product ID
 * NombreProducto — Product Name
 * Venta_uni_hoy — Sales unit this week (integer) 
 * Venta_hoy — Sales this week (unit: pesos)
 * Dev_uni_proxima — Returns unit next week (integer)
 * Dev_proxima — Returns next week (unit: pesos)
 * Demanda_uni_equil — Adjusted Demand (integer) (This is the target you will predict)

##Pré-Analise

Baseado no problema de negocio informado acima, será criado um modelo do tipo de Regressão de Machine Learning de Aprendizado Supervisionado.

Obs: COMO O DATASET É GIGANTE, MAIS DE 3GB COM 74 MILHOES DE OBSERVACOES, DECIDI, GERAR UM DATASET MENOR COM 2 MILHOES DE OBS ESCOLHIDAS ALEATORIAMENTE PARA A ANALISE EXPLORATORIA E FOI CRIADO UM MINI CSV COM 100.000 LINHAS PARA A MONTAGEM DO MODELO PREDITIVO JA RETIRANDO DADOS NA, OS FONTES DOS SPLITS ESTAO CONTIDOS NO ARQUIVO split_dataset.R 

## Etapa Inicial - Carregando as bibliotecas e classes utilitarias
```{r utilidades e bibliotecas}
library(data.table)
library(caret)
library(dplyr)
library(ggplot2)
library(caTools)
library(randomForest)
library(e1071)
library(ROSE)
library(rpart)

source("Utils.R")

```

## Etapa 1 - Coletando os Dados

Aqui está a coleta de dados, neste caso um arquivo csv.

```{r coleta}
#Carregando os dados - Dados de exemplo - 2 Milhoes de obs e com 100.000 linhas
df <- fread("dataset/train_sample.csv", header = T, sep = ";", stringsAsFactors = FALSE)
df2Mini <- fread("dataset/train_mini.csv", header = T, sep = ";", stringsAsFactors = FALSE)
str(df)
View(df)

```

##Analise Exploratoria dos dados

```{r analise}
#Analise Exploratoria dos dados
produtos <- sample(unique(df[,Producto_ID]),10)
list_produtos <- df[Producto_ID %in% produtos]

#Valores positivos da variavel target
list_produtos[,Match:=(Venta_uni_hoy-Dev_uni_proxima)==Demanda_uni_equil]

list_produtos

#Gerando um grafico para visualizar  e explorar melhor os dados
list_produtos %>% 
  ggplot(aes(x=Demanda_uni_equil))+
  geom_histogram(binwidth=2)+
  facet_wrap(~Producto_ID)+
  xlim(c(0,100)) +
  scale_x_continuous(name="")+
  scale_y_continuous(name="")+
  ggtitle("Quantidade de Demanda por Produtos")

#Gerando um grafico para visualizar e explorar melhor os dados
list_produtos %>% 
  ggplot(aes(x=Semana,y=Demanda_uni_equil)) + 
  geom_point(aes(color=factor(Canal_ID))) + 
  facet_wrap(~Producto_ID,scale="free_y") + 
  scale_x_continuous(name="Semana")+
  scale_y_continuous(name="")+
  ggtitle("Demandas de Produto por Semana e Canal")

```

##Data Muning

```{r dataMu}
#Data Muning
#Limpeza dos dados
df2Mini$V1 <- NULL
df2Mini$Cliente_ID <- NULL
df2Mini$Ruta_SAK <- NULL
df2Mini$Agencia_ID <- NULL

#Arrumando os dados
colunasFator <- c("Semana", "Canal_ID")
df2Mini <- to.factors(df2Mini, colunasFator)

#Arrumando Tipagem das Variaveis
df2Mini$Venta_hoy = as.double(sub(",", ".", df2Mini$Venta_hoy))
df2Mini$Dev_proxima = as.double(sub(",", ".", df2Mini$Dev_proxima)) 

#Criando nova coluna com o formula (Venta_uni_hoy - Dev_uni_proxima)
df2Mini <- mutate(df2Mini, quantidade=Venta_uni_hoy - Dev_uni_proxima)

str(df2Mini)

#Mais Limpeza dos dados, deixando no Data Frame, somente as colunas que fazem mais sentido
df2Mini$Venta_uni_hoy <- NULL
df2Mini$Venta_hoy <- NULL
df2Mini$Dev_uni_proxima <- NULL
df2Mini$Dev_proxima <- NULL

```

##Feature Selection
Mapeando as melhores variaveis para o modelo preditivo
```{r variavel}

#Mapeando as melhores variaveis para o modelo preditivo, confirmando as variaveis
modeloSel <- randomForest(Demanda_uni_equil ~ . , 
                          data = df2Mini, 
                          ntree = 15, 
                          nodesize = 10,
                          importance = TRUE)
varImpPlot(modeloSel)

```

## Etapa de Split de dados
Split de dados
```{r split}

#Split de dados
# Funcao para gerar dados de treino e dados de teste
indice = sample.split(df2Mini, SplitRatio = 0.7)

# Gerando dados de treino e de teste - Separando os dados
dados_treino <- df2Mini[indice==TRUE,]
dados_teste <- df2Mini[indice==FALSE,]
class1 <- dados_teste$Demanda_uni_equil
dados_teste$Demanda_uni_equil <- NULL
```

##Construindo o modelo
```{r modelo1}

#Construindo o modelo - Primeiro modelo - Stochastic Gradient Boosting
modFit <- train(Demanda_uni_equil ~ .,method="gbm",data=dados_treino, verbose=FALSE)
modFit

# Testando o modelo com os dados de teste
lr.predictions <- round(predict(modFit, dados_teste, type="raw"))

#Avaliação => 94%
mean(class1 == lr.predictions) 
```
## Modelo 1 - 94% de Acuracia

##Otimizando
```{r modelo2}
#Otimizando o modelo - Segundo modelo - Regressão Linear
modFit2 = lm(Demanda_uni_equil ~ ., 
                data = dados_treino) 

lr.predictions2 <- round(predict(modFit2, dados_teste, type="response"))

#Avaliação => 99%
mean(class1 == lr.predictions2) 
```
## Modelo 2 - 99% de Acuracia

##Segundo modelo foi o melhor

##Tabela Comparativa
Tabela Comparativa 
* gbm - Primeiro Modelo           => 94%
* lm  - Segundo Modelo            => 99%
 
Muito Obrigado. 

Att.

Marcio de Lima

